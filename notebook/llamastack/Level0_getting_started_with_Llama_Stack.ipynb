{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce5ac225-ad97-4cbd-8e90-f0ad64cbf00b",
   "metadata": {},
   "source": [
    "# Level 0: Getting Started with Llama Stack\n",
    "\n",
    "This notebook will help you set up your environment for this tutorial. Specifically, we will cover installing the necessary libraries, configuring essential parameters, and connecting to a Llama Stack server.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "\n",
    "Ensure you have access to a [Llama Stack](https://llama-stack.readthedocs.io/en/latest/) server.\n",
    "\n",
    "If you need to set one up, please follow the instruction set below that is appropriate for your environment:\n",
    "\n",
    "* [Local](../../../local_setup_guide.md) setup guide for a laptop.\n",
    "* [Remote](../../../kubernetes/llama-stack/README.md) setup guide for an OpenShift cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6589b1ae-9fd5-4715-a630-cc3cc52035e9",
   "metadata": {},
   "source": [
    "## Setting the Environment Variables\n",
    "\n",
    "Rename or copy the [`.env.example`](../../../.env.example) file to create a new file called `.env`. We've included as many reasonable defaults as possible to get you started, but please use this file to make any customizations needed for your environment such as the the location of the Llama Stack server endpoint or your personal [Tavily](https://app.tavily.com) api key for web search.  \n",
    "\n",
    "```bash\n",
    "cp .env.example .env\n",
    "```\n",
    "\n",
    "### Environment variables required for all demos\n",
    "- `REMOTE_BASE_URL`: the URL of the remote Llama Stack server.\n",
    "- `TEMPERATURE` (optional): the temperature to use during inference. Defaults to 0.0.\n",
    "- `TOP_P` (optional): the top_p parameter to use during inference. Defaults to 0.95.\n",
    "- `MAX_TOKENS` (optional): the maximum number of tokens that can be generated in the completion. Defaults to 512.\n",
    "- `STREAM` (optional): set this to True to stream the output of the model/agent and False otherwise. Defaults to False.\n",
    "- `VDB_PROVIDER`: the vector DB provider to be used. Must be supported by Llama Stack. For this demo, we use Milvus Lite which is our preferred solution.\n",
    "- `VDB_EMBEDDING`: the embedding model to be used for ingestion and retrieval. For this demo, we use all-MiniLM-L6-v2.\n",
    "- `VDB_EMBEDDING_DIMENSION` (optional): the dimension of the embedding. Defaults to 384.\n",
    "- `VECTOR_DB_CHUNK_SIZE` (optional): the chunk size for the vector DB. Defaults to 512.\n",
    "- `REMOTE_OCP_MCP_URL`: the URL for your Openshift MCP server. If the client does not find the tool registered to the llama-stack instance, it will use this URL to register the Openshift tool.\n",
    "- `REMOTE_SLACK_MCP_URL`: the URL for your Slack MCP server. If the client does not find the tool registered to the llama-stack instance, it will use this URL to register the Slack tool.\n",
    "- `USE_PROMPT_CHAINING`: dictates if the prompt should be formatted as a few separate prompts to isolate each step or in a single turn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573899bf-34cb-4f31-9fc1-48aca439fc60",
   "metadata": {},
   "source": [
    "## Necessary Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "688df15b-3d31-4cbd-b625-9ac91a91983f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dotenv\n",
      "  Downloading dotenv-0.9.9-py2.py3-none-any.whl.metadata (279 bytes)\n",
      "Collecting llama_stack_client\n",
      "  Downloading llama_stack_client-0.2.12-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting fire\n",
      "  Downloading fire-0.7.1-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting python-dotenv (from dotenv)\n",
      "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/app-root/lib64/python3.11/site-packages (from llama_stack_client) (4.9.0)\n",
      "Requirement already satisfied: click in /opt/app-root/lib64/python3.11/site-packages (from llama_stack_client) (8.2.1)\n",
      "Collecting distro<2,>=1.7.0 (from llama_stack_client)\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/app-root/lib64/python3.11/site-packages (from llama_stack_client) (0.28.1)\n",
      "Requirement already satisfied: pandas in /opt/app-root/lib64/python3.11/site-packages (from llama_stack_client) (2.2.3)\n",
      "Requirement already satisfied: prompt-toolkit in /opt/app-root/lib64/python3.11/site-packages (from llama_stack_client) (3.0.51)\n",
      "Collecting pyaml (from llama_stack_client)\n",
      "  Downloading pyaml-25.7.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/app-root/lib64/python3.11/site-packages (from llama_stack_client) (1.10.22)\n",
      "Requirement already satisfied: rich in /opt/app-root/lib64/python3.11/site-packages (from llama_stack_client) (13.9.4)\n",
      "Requirement already satisfied: sniffio in /opt/app-root/lib64/python3.11/site-packages (from llama_stack_client) (1.3.1)\n",
      "Requirement already satisfied: termcolor in /opt/app-root/lib64/python3.11/site-packages (from llama_stack_client) (2.3.0)\n",
      "Requirement already satisfied: tqdm in /opt/app-root/lib64/python3.11/site-packages (from llama_stack_client) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /opt/app-root/lib64/python3.11/site-packages (from llama_stack_client) (4.14.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/app-root/lib64/python3.11/site-packages (from anyio<5,>=3.5.0->llama_stack_client) (3.10)\n",
      "Requirement already satisfied: certifi in /opt/app-root/lib64/python3.11/site-packages (from httpx<1,>=0.23.0->llama_stack_client) (2025.6.15)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/app-root/lib64/python3.11/site-packages (from httpx<1,>=0.23.0->llama_stack_client) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/app-root/lib64/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->llama_stack_client) (0.16.0)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /opt/app-root/lib64/python3.11/site-packages (from pandas->llama_stack_client) (2.2.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/app-root/lib64/python3.11/site-packages (from pandas->llama_stack_client) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/app-root/lib64/python3.11/site-packages (from pandas->llama_stack_client) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/app-root/lib64/python3.11/site-packages (from pandas->llama_stack_client) (2025.2)\n",
      "Requirement already satisfied: wcwidth in /opt/app-root/lib64/python3.11/site-packages (from prompt-toolkit->llama_stack_client) (0.2.13)\n",
      "Requirement already satisfied: PyYAML in /opt/app-root/lib64/python3.11/site-packages (from pyaml->llama_stack_client) (6.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/app-root/lib64/python3.11/site-packages (from rich->llama_stack_client) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/app-root/lib64/python3.11/site-packages (from rich->llama_stack_client) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/app-root/lib64/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->llama_stack_client) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/app-root/lib64/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->llama_stack_client) (1.17.0)\n",
      "Downloading dotenv-0.9.9-py2.py3-none-any.whl (1.9 kB)\n",
      "Downloading llama_stack_client-0.2.12-py3-none-any.whl (340 kB)\n",
      "Downloading fire-0.7.1-py3-none-any.whl (115 kB)\n",
      "Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading pyaml-25.7.0-py3-none-any.whl (26 kB)\n",
      "Downloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
      "Installing collected packages: python-dotenv, pyaml, fire, distro, dotenv, llama_stack_client\n",
      "Successfully installed distro-1.9.0 dotenv-0.9.9 fire-0.7.1 llama_stack_client-0.2.12 pyaml-25.7.0 python-dotenv-1.1.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install dotenv llama_stack_client fire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b09511e-f6dc-4a54-b037-d64adbeaa8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for accessing the environment variables\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# for communication with Llama Stack\n",
    "from llama_stack_client import LlamaStackClient\n",
    "from llama_stack_client.types import UserMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beeb65ed-f368-4bb5-957b-6698fe85b829",
   "metadata": {},
   "source": [
    "## Setting Up the Server Connection\n",
    "\n",
    "Establish the connection to your Llama Stack server.\n",
    "\n",
    "_Note: A Tavily search API key is required for some of our demos and must be provided to the client upon initialization. If you do not have one, you can set one up for free at https://app.tavily.com_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d60fb3f3-4d04-4916-84fc-f798b059ff12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Llama Stack server\n"
     ]
    }
   ],
   "source": [
    "base_url = os.getenv(\"REMOTE_BASE_URL\", \"http://llamastack:8321\")\n",
    "\n",
    "# Tavily search API key is required for some of our demos and must be provided to the client upon initialization.\n",
    "# We will cover it in the agentic demos that use the respective tool. Please ignore this parameter for all other demos.\n",
    "tavily_search_api_key = os.getenv(\"TAVILY_SEARCH_API_KEY\")\n",
    "if tavily_search_api_key is None:\n",
    "    provider_data = None\n",
    "else:\n",
    "    provider_data = {\"tavily_search_api_key\": tavily_search_api_key}\n",
    "\n",
    "\n",
    "client = LlamaStackClient(\n",
    "    base_url=base_url,\n",
    "    provider_data=provider_data\n",
    ")\n",
    "\n",
    "print(f\"Connected to Llama Stack server\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80f5ae8-8083-4896-a973-f310df129ec6",
   "metadata": {},
   "source": [
    "## Initializing the Inference Parameters\n",
    "\n",
    "Fetch the inference-related parameters from the corresponding environment variables and convert them to the format Llama Stack expects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0092359-a5db-4d9a-a735-bb931ba05f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference Parameters:\n",
      "\tSampling Parameters: {'strategy': {'type': 'greedy'}, 'max_tokens': 2048}\n",
      "\tstream: False\n"
     ]
    }
   ],
   "source": [
    "temperature = float(os.getenv(\"TEMPERATURE\", 0.0))\n",
    "if temperature > 0.0:\n",
    "    top_p = float(os.getenv(\"TOP_P\", 0.95))\n",
    "    strategy = {\"type\": \"top_p\", \"temperature\": temperature, \"top_p\": top_p}\n",
    "else:\n",
    "    strategy = {\"type\": \"greedy\"}\n",
    "\n",
    "max_tokens = int(os.getenv(\"MAX_TOKENS\", 9192))\n",
    "\n",
    "# sampling_params will later be used to pass the parameters to Llama Stack Agents/Inference APIs\n",
    "sampling_params = {\n",
    "    \"strategy\": strategy,\n",
    "    \"max_tokens\": max_tokens,\n",
    "}\n",
    "\n",
    "stream_env = os.getenv(\"STREAM\", \"True\")\n",
    "# the Boolean 'stream' parameter will later be passed to Llama Stack Agents/Inference APIs\n",
    "# any value non equal to 'False' will be considered as 'True'\n",
    "stream = (stream_env != \"False\")\n",
    "\n",
    "print(f\"Inference Parameters:\\n\\tSampling Parameters: {sampling_params}\\n\\tstream: {stream}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cdd34a",
   "metadata": {},
   "source": [
    "Now, let's use the Llama stack inference API to greet our LLM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e4423b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://llamastack:8321/v1/inference/chat-completion \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"<think>\\nLet me think about how to respond to this friendly greeting. First, I should acknowledge the greeting in a warm and welcoming way. I want to make sure my response is positive and engaging.\\n\\nI should express that I'm doing well, as that's a common and appropriate response. Then, I can add a bit of personality by mentioning my enthusiasm for our conversation. This helps create a more natural and friendly tone.\\n\\nI should also invite them to share how they're doing, as that's a good way to keep the conversation flowing. I want to be open and approachable, so I'll phrase it in a way that makes them feel comfortable sharing.\\n\\nLet me check if there's anything else I should consider. The response should be concise but not too short, and it should maintain a professional yet personable tone. I don't want to overcomplicate things or add unnecessary information at this stage.\\n\\nOverall, I think a simple, cheerful response that acknowledges their greeting and invites further conversation would be best. It sets a positive tone for our interaction and shows that I'm genuinely interested in what they have to say.\\n</think>\\n\\nHello! I'm doing great, thanks for asking! I'm always excited to chat and help out. How are you feeling today? I'd love to hear how you're doing! ðŸ˜Š\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message = UserMessage(\n",
    "    content=\"Hi, how are you?\",\n",
    "    role=\"user\",\n",
    ")\n",
    "client.inference.chat_completion(\n",
    "    model_id=\"qwen\",\n",
    "    messages=[message],\n",
    "    sampling_params=sampling_params,\n",
    "    stream=stream\n",
    ").completion_message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4",
   "metadata": {},
   "source": [
    "## Bonus: Customizing LLM Responses\n",
    "\n",
    "Let's explore how to customize the LLM's response style. The cell below asks a simple question that any LLM would know. We've provided two versions of the prompt - a regular one and a pirate-themed one. Try commenting out the regular prompt and uncommenting the pirate version to see how we can change the personality of the response! You will need to restart to see the changes if the notebook has already been executed.\n",
    "\n",
    "Feel free to:\n",
    "- Switch between the two prompts by commenting/uncommenting\n",
    "- Change the question to anything you'd like\n",
    "- Create your own personality styles (try a medieval knight, a robot, or a Shakespeare character!)\n",
    "- Experiment with different prompting techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5f6g7h8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://llamastack:8321/v1/inference/chat-completion \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so the user is asking, \"What is the capital of France?\" Hmm, I need to make sure I get this right. Let me think. I remember from school that France is a country in Europe, and their capital is a major city. I think it's Paris. Wait, is that correct? Let me verify. I've heard of Paris being known for the Eiffel Tower and the Louvre Museum. Yeah, that sounds right. But wait, sometimes people confuse capitals with other big cities. For example, some countries have capitals that aren't their largest cities, but in France's case, Paris is both the capital and the largest city. I don't think there's any other city that's commonly mistaken for the capital. Let me think of other European capitals. Germany's is Berlin, Italy's is Rome, Spain's is Madrid. So France's should be Paris. I'm pretty confident about that. But just to be thorough, maybe I should recall some historical context. Paris has been the capital for a long time, right? Even during different regimes and governments in French history, Paris remained the capital. I don't think there was a period when the capital was moved elsewhere. So yeah, the answer is definitely Paris. I don't see any reason to doubt that. The user probably expects a straightforward answer here, so I should just state it clearly.\n",
      "</think>\n",
      "\n",
      "The capital of France is **Paris**. It is not only the political and administrative center of the country but also renowned for its cultural landmarks, such as the Eiffel Tower, the Louvre Museum, and Notre-Dame Cathedral. Paris has served as France's capital for centuries and remains its largest city.\n"
     ]
    }
   ],
   "source": [
    "# Feel free to change this question to anything you'd like!\n",
    "# Uncomment one of the prompts below:\n",
    "\n",
    "# Regular version:\n",
    "prompt = \"What is the capital of France?\"\n",
    "\n",
    "# Pirate version (uncomment this line and comment out the line above):\n",
    "# prompt = \"Please answer the following question as if you were a pirate captain: What is the capital of France?\"\n",
    "\n",
    "# Create the message\n",
    "message = UserMessage(\n",
    "    content=prompt,\n",
    "    role=\"user\",\n",
    ")\n",
    "\n",
    "# Get the response\n",
    "response = client.inference.chat_completion(\n",
    "    model_id=\"qwen\",\n",
    "    messages=[message],\n",
    "    sampling_params=sampling_params,\n",
    "    stream=stream\n",
    ")\n",
    "\n",
    "print(response.completion_message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d0a375",
   "metadata": {},
   "source": [
    "# Next\n",
    "\n",
    "Now that we've set up our Tutorial environment, Let's get started building with Llama Stack! The next notebook will teach you how to build a [Simple RAG](./Level1_simple_RAG.ipynb) application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1219179",
   "metadata": {},
   "source": [
    "#### Any Feedback?\n",
    "\n",
    "If you have any feedback on this or any other notebook in this demo series we'd love to hear it! Please go to https://www.feedback.redhat.com/jfe/form/SV_8pQsoy0U9Ccqsvk and help us improve our demos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a017e6f2-299c-48bb-bd16-174987e98ef7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
