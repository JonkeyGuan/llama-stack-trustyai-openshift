{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dotenv in /opt/app-root/lib64/python3.11/site-packages (0.9.9)\n",
      "Collecting llama_stack_client\n",
      "  Downloading llama_stack_client-0.2.12-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting fire\n",
      "  Downloading fire-0.7.1-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: python-dotenv in /opt/app-root/lib64/python3.11/site-packages (from dotenv) (1.1.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/app-root/lib64/python3.11/site-packages (from llama_stack_client) (4.9.0)\n",
      "Requirement already satisfied: click in /opt/app-root/lib64/python3.11/site-packages (from llama_stack_client) (8.2.1)\n",
      "Collecting distro<2,>=1.7.0 (from llama_stack_client)\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/app-root/lib64/python3.11/site-packages (from llama_stack_client) (0.28.1)\n",
      "Requirement already satisfied: pandas in /opt/app-root/lib64/python3.11/site-packages (from llama_stack_client) (2.2.3)\n",
      "Requirement already satisfied: prompt-toolkit in /opt/app-root/lib64/python3.11/site-packages (from llama_stack_client) (3.0.51)\n",
      "Collecting pyaml (from llama_stack_client)\n",
      "  Downloading pyaml-25.7.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/app-root/lib64/python3.11/site-packages (from llama_stack_client) (1.10.22)\n",
      "Requirement already satisfied: rich in /opt/app-root/lib64/python3.11/site-packages (from llama_stack_client) (13.9.4)\n",
      "Requirement already satisfied: sniffio in /opt/app-root/lib64/python3.11/site-packages (from llama_stack_client) (1.3.1)\n",
      "Requirement already satisfied: termcolor in /opt/app-root/lib64/python3.11/site-packages (from llama_stack_client) (2.3.0)\n",
      "Requirement already satisfied: tqdm in /opt/app-root/lib64/python3.11/site-packages (from llama_stack_client) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /opt/app-root/lib64/python3.11/site-packages (from llama_stack_client) (4.14.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/app-root/lib64/python3.11/site-packages (from anyio<5,>=3.5.0->llama_stack_client) (3.10)\n",
      "Requirement already satisfied: certifi in /opt/app-root/lib64/python3.11/site-packages (from httpx<1,>=0.23.0->llama_stack_client) (2025.6.15)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/app-root/lib64/python3.11/site-packages (from httpx<1,>=0.23.0->llama_stack_client) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/app-root/lib64/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->llama_stack_client) (0.16.0)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /opt/app-root/lib64/python3.11/site-packages (from pandas->llama_stack_client) (2.2.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/app-root/lib64/python3.11/site-packages (from pandas->llama_stack_client) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/app-root/lib64/python3.11/site-packages (from pandas->llama_stack_client) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/app-root/lib64/python3.11/site-packages (from pandas->llama_stack_client) (2025.2)\n",
      "Requirement already satisfied: wcwidth in /opt/app-root/lib64/python3.11/site-packages (from prompt-toolkit->llama_stack_client) (0.2.13)\n",
      "Requirement already satisfied: PyYAML in /opt/app-root/lib64/python3.11/site-packages (from pyaml->llama_stack_client) (6.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/app-root/lib64/python3.11/site-packages (from rich->llama_stack_client) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/app-root/lib64/python3.11/site-packages (from rich->llama_stack_client) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/app-root/lib64/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->llama_stack_client) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/app-root/lib64/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->llama_stack_client) (1.17.0)\n",
      "Downloading llama_stack_client-0.2.12-py3-none-any.whl (340 kB)\n",
      "Downloading fire-0.7.1-py3-none-any.whl (115 kB)\n",
      "Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading pyaml-25.7.0-py3-none-any.whl (26 kB)\n",
      "Installing collected packages: pyaml, fire, distro, llama_stack_client\n",
      "Successfully installed distro-1.9.0 fire-0.7.1 llama_stack_client-0.2.12 pyaml-25.7.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install dotenv llama_stack_client fire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import rich\n",
    "\n",
    "from llama_stack_client import LlamaStackClient\n",
    "from llama_stack_client.types import UserMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "base_url = os.getenv(\"REMOTE_BASE_URL\", \"http://localhost:8321\")\n",
    "tavily_search_api_key = os.getenv(\"TAVILY_SEARCH_API_KEY\", \"dummy\")\n",
    "\n",
    "provider_data = {\"tavily_search_api_key\": tavily_search_api_key}\n",
    "client = LlamaStackClient(\n",
    "    base_url=base_url,\n",
    "    provider_data=provider_data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://llamastack:8321/v1/inference/chat-completion \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatCompletionResponse</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">completion_message</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">CompletionMessage</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"I'm an artificial intelligence and don't have feelings, but I'm here to help you. How can I assist</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">you today?\"</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">role</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'assistant'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">stop_reason</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'end_of_turn'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">tool_calls</span>=<span style=\"font-weight: bold\">[]</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">logprobs</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">metrics</span>=<span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Metric</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">metric</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'prompt_tokens'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">value</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">unit</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span><span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Metric</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">metric</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'completion_tokens'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">value</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">unit</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span><span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Metric</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">metric</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">value</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">unit</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span><span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">]</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mChatCompletionResponse\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mcompletion_message\u001b[0m=\u001b[1;35mCompletionMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mcontent\u001b[0m=\u001b[32m\"I\u001b[0m\u001b[32m'm an artificial intelligence and don't have feelings, but I'm here to help you. How can I assist\u001b[0m\n",
       "\u001b[32myou today?\"\u001b[0m,\n",
       "        \u001b[33mrole\u001b[0m=\u001b[32m'assistant'\u001b[0m,\n",
       "        \u001b[33mstop_reason\u001b[0m=\u001b[32m'end_of_turn'\u001b[0m,\n",
       "        \u001b[33mtool_calls\u001b[0m=\u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[33mlogprobs\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "    \u001b[33mmetrics\u001b[0m=\u001b[1m[\u001b[0m\n",
       "        \u001b[1;35mMetric\u001b[0m\u001b[1m(\u001b[0m\u001b[33mmetric\u001b[0m=\u001b[32m'prompt_tokens'\u001b[0m, \u001b[33mvalue\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m, \u001b[33munit\u001b[0m=\u001b[3;35mNone\u001b[0m\u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mMetric\u001b[0m\u001b[1m(\u001b[0m\u001b[33mmetric\u001b[0m=\u001b[32m'completion_tokens'\u001b[0m, \u001b[33mvalue\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m, \u001b[33munit\u001b[0m=\u001b[3;35mNone\u001b[0m\u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mMetric\u001b[0m\u001b[1m(\u001b[0m\u001b[33mmetric\u001b[0m=\u001b[32m'total_tokens'\u001b[0m, \u001b[33mvalue\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m, \u001b[33munit\u001b[0m=\u001b[3;35mNone\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m]\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "strategy = {\"type\": \"greedy\"}\n",
    "max_tokens = int(os.getenv(\"MAX_TOKENS\", 512))\n",
    "sampling_params = {\n",
    "    \"strategy\": strategy,\n",
    "    \"max_tokens\": max_tokens,\n",
    "}\n",
    "\n",
    "stream=False\n",
    "\n",
    "message = UserMessage(\n",
    "    content=\"Hi, how are you?\",\n",
    "    role=\"user\"\n",
    ")\n",
    "\n",
    "response = client.inference.chat_completion(\n",
    "    model_id=\"granite\",\n",
    "    messages=[message],\n",
    "    sampling_params=sampling_params,\n",
    "    stream=stream\n",
    ")\n",
    "\n",
    "rich.print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://llamastack:8321/v1/inference/chat-completion \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatCompletionResponse</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">completion_message</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">CompletionMessage</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'&lt;think&gt;\\nOkay, the user greeted me with \"Hi, how are you?\" I need to respond in a friendly and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">helpful manner. Let me start by acknowledging their greeting. I should mention that I\\'m just a chatbot and don\\'t </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">have feelings, but I\\'m here to help. Maybe add a bit of personality to keep it engaging. Also, I should invite </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">them to ask questions or share what they need assistance with. Keep it simple and conversational. Let me check for </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">any errors and make sure the tone is positive.\\n&lt;/think&gt;\\n\\nHello! I\\'m just a chatbot, so I don\\'t have feelings, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">but I\\'m here and ready to help! ðŸ˜Š How can I assist you today? Whether you have questions, need advice, or just </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">want to chat, I\\'m all ears! What\\'s on your mind?'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">role</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'assistant'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">stop_reason</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'end_of_turn'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">tool_calls</span>=<span style=\"font-weight: bold\">[]</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">logprobs</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">metrics</span>=<span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Metric</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">metric</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'prompt_tokens'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">value</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">unit</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span><span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Metric</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">metric</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'completion_tokens'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">value</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">unit</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span><span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Metric</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">metric</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">value</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">unit</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span><span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">]</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mChatCompletionResponse\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mcompletion_message\u001b[0m=\u001b[1;35mCompletionMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mcontent\u001b[0m=\u001b[32m'\u001b[0m\u001b[32m<\u001b[0m\u001b[32mthink\u001b[0m\u001b[32m>\\nOkay, the user greeted me with \"Hi, how are you?\" I need to respond in a friendly and \u001b[0m\n",
       "\u001b[32mhelpful manner. Let me start by acknowledging their greeting. I should mention that I\\'m just a chatbot and don\\'t \u001b[0m\n",
       "\u001b[32mhave feelings, but I\\'m here to help. Maybe add a bit of personality to keep it engaging. Also, I should invite \u001b[0m\n",
       "\u001b[32mthem to ask questions or share what they need assistance with. Keep it simple and conversational. Let me check for \u001b[0m\n",
       "\u001b[32many errors and make sure the tone is positive.\\n</think\u001b[0m\u001b[32m>\u001b[0m\u001b[32m\\n\\nHello! I\\'m just a chatbot, so I don\\'t have feelings, \u001b[0m\n",
       "\u001b[32mbut I\\'m here and ready to help! ðŸ˜Š How can I assist you today? Whether you have questions, need advice, or just \u001b[0m\n",
       "\u001b[32mwant to chat, I\\'m all ears! What\\'s on your mind?'\u001b[0m,\n",
       "        \u001b[33mrole\u001b[0m=\u001b[32m'assistant'\u001b[0m,\n",
       "        \u001b[33mstop_reason\u001b[0m=\u001b[32m'end_of_turn'\u001b[0m,\n",
       "        \u001b[33mtool_calls\u001b[0m=\u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[33mlogprobs\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "    \u001b[33mmetrics\u001b[0m=\u001b[1m[\u001b[0m\n",
       "        \u001b[1;35mMetric\u001b[0m\u001b[1m(\u001b[0m\u001b[33mmetric\u001b[0m=\u001b[32m'prompt_tokens'\u001b[0m, \u001b[33mvalue\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m, \u001b[33munit\u001b[0m=\u001b[3;35mNone\u001b[0m\u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mMetric\u001b[0m\u001b[1m(\u001b[0m\u001b[33mmetric\u001b[0m=\u001b[32m'completion_tokens'\u001b[0m, \u001b[33mvalue\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m, \u001b[33munit\u001b[0m=\u001b[3;35mNone\u001b[0m\u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mMetric\u001b[0m\u001b[1m(\u001b[0m\u001b[33mmetric\u001b[0m=\u001b[32m'total_tokens'\u001b[0m, \u001b[33mvalue\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m, \u001b[33munit\u001b[0m=\u001b[3;35mNone\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m]\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "message = UserMessage(\n",
    "    content=\"Hi, how are you?\",\n",
    "    role=\"user\"\n",
    ")\n",
    "\n",
    "response = client.inference.chat_completion(\n",
    "    model_id=\"qwen\",\n",
    "    messages=[message],\n",
    "    sampling_params=sampling_params,\n",
    ")\n",
    "\n",
    "rich.print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
